---
title: "BodyFat"
author: "Xintong Shi"
date: "10/1/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(reshape2)
library(ggplot2)
library(plyr)
library(readr)
library(dplyr)
library(caret)
library(ggplot2)
library(glmnet)
library(randomForest)
#library(repr)
```

# Body Fat Prediction

## Exploratory Data Analysis

### Data Cleaning
```{r}
data = read.csv("BodyFat.csv")
```

```{r}
#check data
data
```
```{r}
#remove 0 bodyfat
data = data[-which(data$BODYFAT==0),]
data = data[-39,]
```

```{r}
#remove the ID numbers
data$IDNO <- NULL
```

```{r}
#Summary of distribution
summary(data)
```

```{r}
#check for NA
sum(is.na(data))
```

```{r}
write.csv(data,"BodyFat_clean.csv", row.names = FALSE)
```


### Check correlation

```{r}
#correlation matrix
cormat <- round(cor(data),2)
head(cormat)
```

```{r}
melted_cormat <- melt(cormat)
```

```{r}
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value),label=round(r_if_sig,2)) + 
  geom_tile()+theme(axis.text.x=element_text(angle = 45, vjust = 0.5)) +
  labs(x = NULL, y = NULL, fill = "Pearson's\nCorrelation", title="Correlations in BodyFat variables",   subtitle="") + scale_fill_gradient2(mid="#FBFEF9",low="#0C6291",high="#A63446", limits=c(-1,1)) +
 scale_x_discrete(expand=c(0,0)) +
 scale_y_discrete(expand=c(0,0))
```

## Modeling & Variable Importance Analysis

```{r}
set.seed(100) 

index = sample(1:nrow(data), 0.7*nrow(data)) 

train = data[index,] # Create the training data 
test = data[-index,] # Create the test data

dim(train)
dim(test)
```

```{r}
#simple linear model
simple_lr = lm(BODYFAT~ ., data=train)
summary(simple_lr)
```
```{r}
predictions_train = predict(simple_lr, newdata = train)
sqrt(mean((train$BODYFAT - predictions_train)^2))
predictions_test = predict(simple_lr, newdata = test)
sqrt(mean((test$BODYFAT - predictions_test)^2))
```



```{r}
cols_reg = colnames(data)

dummies <- dummyVars(BODYFAT ~ ., data = data[,cols_reg])

train_dummies = predict(dummies, newdata = train[,cols_reg])

test_dummies = predict(dummies, newdata = test[,cols_reg])

print(dim(train_dummies))
print(dim(test_dummies))
```
```{r}
x = as.matrix(train_dummies)
y_train = train$BODYFAT

x_test = as.matrix(test_dummies)
y_test = test$BODYFAT
```
### Ridge regression


Tuning for optimal lambda

```{r}
lambdas <- 10^seq(2, -3, by = -.1)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
```

```{r}
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = 0.1258925)

coef(ridge_reg)
```

```{r}
#train prediction
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
sqrt(mean((y_train - predictions_train)^2))
```

```{r}
#test prediction
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
sqrt(mean((y_test - predictions_test)^2))
```

### Lasso Regression

```{r}
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)
lambda_best <- lasso_reg$lambda.min 
lambda_best
```
```{r}
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)
coef(lasso_model)
```

```{r}
#test prediction
predictions_train <- predict(lasso_model, s = optimal_lambda, newx = x)
sqrt(mean((y_train - predictions_train)^2))
```

```{r}
#test prediction
predictions_test <- predict(lasso_model, s = optimal_lambda, newx = x_test)
sqrt(mean((y_test - predictions_test)^2))
```


### RandomForest
```{r}
bodyfat.rf <- randomForest(BODYFAT ~ ., data=data,mtry=3, importance=TRUE,na.action=na.omit)
print(bodyfat.rf)
```

```{r}
round(importance(bodyfat.rf), 2)
```
```{r}
hist(treesize(bodyfat.rf),
     main = "No. of Nodes for the Trees",
     col = "green")

varImpPlot(bodyfat.rf,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")
importance(bodyfat.rf)

```

From Random Forest's variable importance graph, combined with Lasso's feature selection, we found that AGE,ABDOMEN, ADIPOSITY, CHEST, DENSITY to be the most important variables. After performing collinearity analysis, we remove ABDOMEN due to its high VIF score with the other variables. Furthermore, we remove DENSITY due to large coefficient.

## Final Model

### Simple linear regression
```{r}
##selected linear model
lr_final = lm(BODYFAT ~ AGE + ADIPOSITY + CHEST, data=train)
summary(lr_final)
```


```{r}
predictions_train = predict(lr_final, newdata = train)
sqrt(mean((train$BODYFAT - predictions_train)^2))
predictions_test = predict(lr_final, newdata = test)
sqrt(mean((test$BODYFAT - predictions_test)^2))
```

### Ridge Regression

```{r}
#selecting only 4 variables
cols_reg_final = c("AGE","ADIPOSITY","CHEST","BODYFAT")

dummies_final <- dummyVars(BODYFAT ~ ., data = data[,cols_reg_final])

train_dummies_final = predict(dummies_final, newdata = train[,cols_reg_final])

test_dummies_final = predict(dummies_final, newdata = test[,cols_reg_final])

x_final = as.matrix(train_dummies_final)
y_final_train = train$BODYFAT

x_final_test = as.matrix(test_dummies_final)
y_final_test = test$BODYFAT
```


```{r}
ridge_reg_final = glmnet(x_final, y_final_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = 0.1258925)

coef(ridge_reg_final)
```

